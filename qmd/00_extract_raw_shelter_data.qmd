---
title: "Data Extraction 1 - Raw Shelter Data"
date: "`r lubridate::now(tzone = 'EST')` EST"
format: html
theme: flatly
toc: true
toc-depth: 3
number-sections: true
number-depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Libraries
library(tidyverse)
library(opendatatoronto)
library(bigrquery)

# Source
source(file = "../functions/extract_shelter_data.R")
```


# Introduction

This notebook uses the [opendatatoronto](https://github.com/sharlagelfand/opendatatoronto) R package to extract daily data on shelter occupancy and overnight service occupancy for the city of Toronto. The package pulls data from [Toronto open data](https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/) which provides an API to access shelter occupancy data.

Next the data is loaded to a [BigQuery](https://cloud.google.com/bigquery) project for further transformation using [DBT](https://docs.getdbt.com/)

---

# Get Data from API 

The `get_shelter_data()` function is designed to retrieve shelter data from an API
The function primarily focuses on extracting data in CSV or GeoJSON format, ensuring that 
the datasets are not only available but also recently modified, thereby providing 
the most up-to-date information available.

The function operates in two main steps:

1. Information Retrieval: It first retrieves metadata about available resources 
using show_package() and list_package_resources(), filtering for relevant 
data formats and ensuring that the data is recent by checking the last_modified 
field. The datasets are then arranged in descending order based on their 
last modification date to prioritize the most recent data.

2. Data Extraction and Cleaning: The function then extracts the specified slice 
of data using slice() and get_resource(), and cleans the variable names 
using janitor::clean_names() to ensure they are syntactically valid and 
easy to work with in R.

---

## Get 2022 Daily Data from API

This part of the workflow is a static one time extract and load, since this is 2022
historical data and will not be changing. This code chunk will remain commented out
after the first extraction.

```{r extract_shelter_data_2022}

# 2022 daily data
# daily_shelter_2022_tbl <- get_shelter_data(slice = 3)
# 
# daily_shelter_2022_tbl %>% glimpse()

```


---

## Get Latest 2023 Daily Data from API

Get latest 2023 data.

```{r extract_shelter_data_2023}

# 2022 daily data
daily_shelter_2023_tbl <- get_shelter_data(slice = 1)

daily_shelter_2023_tbl %>% glimpse()
```


---

# Load Data to BigQuery

The `get_bigquery_upload()` function is designed to facilitate the uploading of data 
to Google BigQuery It aims to streamline the process of data upload by 
establishing a connection to BigQuery, initiating an upload job, and providing 
feedback about the job status and timings.

The function performs several key operations:
1. Parameter Validation: It validates the input parameters to ensure they are 
of the correct type and non-empty, preventing unintended operations or unclear 
error messages during the upload process.
2. BigQuery Connection: Establishes a connection to Google BigQuery, specifying 
the project and dataset to be used, and assuming the billing to be charged 
to the specified project.
3. Data Upload: Initiates an upload job to insert the data into the specified 
table in BigQuery, with flexibility in handling table creation disposition.
4. Feedback Provision: Provides feedback about the job status, ID, creation time, 
and start time, aiding in monitoring and debugging.

---

## Load 2022 Data (One Time)

Upload 2022 data. Again this is a one time upload. 

```{r upload_shelter_data_2022}
# 
# # 2022 data (one time upload)
# get_bigquery_upload(
#     values  = daily_shelter_2022_tbl,
#     project = "toronto-shelter-project",
#     dataset = "data_raw",
#     table   = "raw_shelter_2022"
# 
# )

```

---

## Load 2023 Data (On Going)

Upload 2023 data. Again this is a one time upload. 

```{r, upload_shelter_data_2023, message=TRUE}

# 2022 data (one time upload)
get_bigquery_upload(
    values  = daily_shelter_2023_tbl,
    project = "toronto-shelter-project",
    dataset = "data_raw",
    table   = "raw_shelter_2023"
    
)

```
