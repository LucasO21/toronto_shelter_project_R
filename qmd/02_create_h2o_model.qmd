---
title: "Data Modeling - Build H2o AutoML Models"
date: "`r lubridate::now(tzone = 'EST')` EST"
format: html
theme: flatly
toc: true
toc-depth: 3
number-sections: true
number-depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Libraries
library(tidyverse)
library(bigrquery)
library(slider)
library(janitor)
library(tidymodels)
library(h2o)

# Source
source(file = "../functions/extract_shelter_data.R")
source(file = "../functions/extract_weather_forecast.R")
source(file = "../functions/modeling.R")
```


# Introduction

In this notebook, we employ H2O AutoML to develop two distinct machine learning models 
aimed at forecasting overnight shelter occupancy. The first model, a classification model, 
predicts whether a specific location will be fully occupied (100%) based on certain features.
The second, a regression model, estimates the actual number of occupied beds or rooms, 
also based on certain features. Collectively, these models assist in 
accurately predicting future shelter occupancy levels.

---

# Get Data Biq Query

Get the data for modeling from biq query.

```{r load_data, message=TRUE}

# Get Data from BQ

# 2022
features_raw_list_2022 <- get_features_data_from_bigquery(year = 2022)

# 2023
features_raw_list_2023 <- get_features_data_from_bigquery(year = 2023)

```

---

## Prep Weather Data

For data in 2023, there is a difference of about 3 days between shelter data and weather data. We want
to fill in temp values for these days. The `get_weather_data_prepped()` function
does this step.

## Prep Weather Data (2023 Only)

This step prepares the weather data by filling the gap stated earlier with a rolling average 
of weather values for the last 3 days.


```{r prep_weather_data_2023}

# Prep Weather Data 2023 Only
daily_weather_2023_prepped_tbl <- get_weather_data_prepped(features_raw_list_2023)
```


---
## Combine Data Step 1: Shelter Data & Weather Data

This step combines the shelter data with weather for 2022 and 2023.

```{r combine_data_1}

# 2022
daily_combined_tbl_2022 <- features_raw_list_2022[[1]] %>% 
  filter(modeling_cohort == 1) %>% 
  left_join(
    daily_weather_2023_prepped_tbl %>% 
      select(-stn),
    by = c("occupancy_date" = "date")
  ) %>% 
  filter(occupancy_date >= as.Date("2022-10-01"))


# 2023
daily_combined_tbl_2023 <- features_raw_list_2023[[1]] %>% 
  #select(occupancy_date) %>% 
  left_join(
    get_weather_data_prepped(features_raw_list_2023) %>% select(-stn),
    by = c("occupancy_date" = "date")
  )

```


---
## Combine Data Step 2: 2022 & 2023

This step combines 2022 and 2023 data.

```{r combine_data_2}


```



---
# Data Filtering

To test out the workflow, we want to limit the number of locations we model. This test
will focus on modeling the shelters with the highest number of locations. 

```{r top_locations}

# Show Top Locations
daily_shelter_tbl %>% 
    summarise(
       n   = n_distinct(location_id),
       .by = c(organization_name, organization_id)
    ) %>% 
    arrange(desc(n)) %>% 
    head(5)

```

Notice the top 3 organizations in terms of the number of distinct locations. Therefore
we'll focus on City of Toronto (22 locations), Homes First Society (10 locations) and
Dixon Hall (6 locations).

---

## Data Filtering for Top 3 Organizations/Locations

```{r data_filtering}

# Data Filtering
sample_shelter_tbl <- daily_shelter_tbl %>% 
  filter(organization_id %in% c(1, 15, 6)) %>% 
  filter(modeling_cohort == 1) # this step is filtering for locations open > 75% of days in year

sample_shelter_tbl %>% glimpse()

```

---

# Get Modeling Features

Extract the necessary features needed for modeling

```{r modeling_features}

# Get Modeling Features
modeling_tbl <- get_modeling_features(sample_shelter_tbl)

modeling_tbl %>% glimpse()

```


---

# Data Prep for Modeling

## Data Splitting

80/20 training/testing split

```{r}
set.seed(123)
splits_spec <- initial_split(modeling_tbl, prop = 0.80, strata = occupancy_rate)

train_raw_tbl <- training(splits_spec)
test_raw_tbl  <- testing(splits_spec)
```

## Create Recipes

The function `get_automl_recipes()` is used to build classification and regression
recipes.

```{r recipes}

# Create Recipes
recipe_spec_prob <- get_automl_recipes(train_raw_tbl)

recipe_spec_reg <- get_automl_recipes(train_raw_tbl, prob = FALSE)

recipe_spec_prob

recipe_spec_reg

```

---

# Model Training (H2o AutoML)

Modeling with H2o.

```{r h2o_init}

# Initialize H2o
h2o.init()

```

## Classification Model

```{r prob_model}

# Automl Models Prob
automl_output_list_prob <- get_automl_models(
  recipe     = recipe_spec_prob,
  train_data = train_raw_tbl,
  test_data  = test_raw_tbl,
  prob       = TRUE
)

automl_output_list_prob
```


## Regression Model

```{r reg_model}
# Automl Models Reg
automl_output_list_reg<- get_automl_models(
  recipe     = recipe_spec_reg,
  train_data = train_raw_tbl,
  test_data  = test_raw_tbl,
  prob       = FALSE
)

automl_output_list_reg
```

## Save AutoML Artifacts

```{r save_artifacts}

# Save Best Model (Prob)
automl_output_list_prob %>% write_rds("../artifacts/h2o_models_v1/automl_list_prob.rds")

# Save Best Model (Reg)
automl_output_list_reg %>% write_rds("../artifacts/h2o_models_v1/automl_list_reg.rds")

```

